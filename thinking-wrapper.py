#!/usr/bin/env python3
"""
thinking-wrapper.py â€” SSE Streaming + Thinking + Codex Effort ë³€í™˜ í”„ë¡ì‹œ

ì§€ì› ì—”ë“œí¬ì¸íŠ¸:
  /v1/chat/completions â€” OpenAI í˜•ì‹ (VS Code Copilot BYOK ë“±)
  /v1/messages         â€” Anthropic í˜•ì‹ (Claude Code CLI ë“±)

ì§€ì›í•˜ëŠ” ë³€í™˜:
1. Claude Thinking ëª¨ë¸: thinking íŒŒë¼ë¯¸í„° ìë™ ì‚½ì…
2. Codex Effort ëª¨ë¸: effort ì ‘ë¯¸ì‚¬ íŒŒì‹± â†’ reasoning_effort ì‚½ì…
3. ë‚˜ë¨¸ì§€ ëª¨ë¸: CCS íŒ¨ìŠ¤ìŠ¤ë£¨

Usage:
    python3 thinking-wrapper.py [--port 8318]
"""

import json
import time
import httpx
from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse, StreamingResponse
import uvicorn
import argparse
import re

app = FastAPI(title="CCS Thinking + Effort Wrapper")

CCS_BASE = "http://localhost:8317"
CCS_API_KEY = "ccs-internal-managed"

# Claude thinking ëª¨ë¸: Anthropic Messagesë¡œ ë³€í™˜
THINKING_MODELS = {
    "claude-opus-4-6-thinking":   {"effort": "max", "max_tokens": 128000},
    "claude-sonnet-4-5-thinking": {"effort": "high", "max_tokens": 64000},
}

# ëª¨ë¸ ë³„ì¹­: Claude Code ë‚´ë¶€ ëª¨ë¸ëª… â†’ CCS ì‹¤ì œ ëª¨ë¸ëª…
# Claude CodeëŠ” Haiku ìŠ¬ë¡¯ì„ ë¶„ë¥˜/ì¹´ìš´íŒ…ì— ì‚¬ìš© â†’ Sonnet 4.6ìœ¼ë¡œ ì—…ê·¸ë ˆì´ë“œ
MODEL_ALIASES = {
    # Haiku ìŠ¬ë¡¯ â†’ Sonnet 4.6 ì—…ê·¸ë ˆì´ë“œ
    "claude-haiku-4-5-20251001": "claude-sonnet-4-6",
    "claude-haiku-4-5":          "claude-sonnet-4-6",
    # Sonnet ìŠ¬ë¡¯ (ê¸°ë³¸ ëª¨ë¸) â†’ Codex xhigh ë¦¬ë§µ
    "claude-sonnet-4-5-20250929": "gpt-5.3-codex-xhigh",
    "claude-sonnet-4-5":          "gpt-5.3-codex-xhigh",
    "claude-sonnet-4":            "gpt-5.3-codex-xhigh",
}

# Codex effort ì ‘ë¯¸ì‚¬ íŒ¨í„´
EFFORT_SUFFIXES = re.compile(r"^(.+)-(xhigh|high|medium|low)$")

# effort ë§¤í•‘
EFFORT_MAP = {
    "low": "low", "medium": "medium", "high": "high",
    "max": "max", "xhigh": "xhigh",
}

CCS_HEADERS = {
    "Authorization": f"Bearer {CCS_API_KEY}",
    "Content-Type": "application/json",
}


@app.get("/v1/models")
async def list_models():
    """CCSì˜ ëª¨ë¸ ëª©ë¡ì„ ê·¸ëŒ€ë¡œ ì „ë‹¬"""
    async with httpx.AsyncClient(timeout=10) as client:
        r = await client.get(
            f"{CCS_BASE}/v1/models",
            headers={"Authorization": f"Bearer {CCS_API_KEY}"},
        )
        return JSONResponse(content=r.json(), status_code=r.status_code)


@app.post("/v1/chat/completions")
async def chat_completions(request: Request):
    body = await request.json()
    model = body.get("model", "")
    is_stream = body.get("stream", False)

    print(f"ğŸ“¨ {model} stream={is_stream} msgs={len(body.get('messages',[]))} tools={len(body.get('tools',[]))}")

    # --- Route 1: Claude Thinking ëª¨ë¸ ---
    if model in THINKING_MODELS:
        return await _handle_thinking(body, model, is_stream)

    # --- Route 2: Codex Effort ì ‘ë¯¸ì‚¬ ëª¨ë¸ ---
    match = EFFORT_SUFFIXES.match(model)
    if match:
        base_model = match.group(1)
        effort = match.group(2)
        return await _handle_codex_effort(body, base_model, effort)

    # --- Route 3: ì¼ë°˜ ëª¨ë¸ passthrough (SSE stream) ---
    return await _stream_passthrough(f"{CCS_BASE}/v1/chat/completions", body)


async def _stream_passthrough(url: str, body: dict):
    """CCSì˜ SSE ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ê·¸ëŒ€ë¡œ Copilotì— ì „ë‹¬"""
    body["stream"] = True  # CCSì— ìŠ¤íŠ¸ë¦¬ë° ê°•ì œ

    async def generate():
        async with httpx.AsyncClient(timeout=300) as client:
            async with client.stream(
                "POST", url, json=body, headers=CCS_HEADERS,
            ) as response:
                async for line in response.aiter_lines():
                    if line:
                        yield line + "\n"
                    else:
                        yield "\n"

    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
        },
    )


async def _handle_codex_effort(body: dict, base_model: str, effort: str):
    """Codex effort ëª¨ë¸: ì ‘ë¯¸ì‚¬ íŒŒì‹± â†’ reasoning_effort ì‚½ì… â†’ SSE passthrough"""
    body["model"] = base_model
    body["reasoning_effort"] = effort

    # VS Code Copilotì´ ë³´ë‚´ëŠ” ë¶ˆí•„ìš”í•œ í•„ë“œ ì œê±°
    for key in ["n", "logprobs", "top_logprobs", "response_format"]:
        body.pop(key, None)

    print(f"ğŸ”§ Codex effort: {base_model} + {effort}")
    return await _stream_passthrough(
        f"{CCS_BASE}/api/provider/codex/v1/chat/completions", body
    )


async def _handle_thinking(body: dict, model: str, is_stream: bool):
    """Claude thinking ëª¨ë¸: Anthropic Messages â†’ OpenAI SSE ë³€í™˜"""
    config = THINKING_MODELS[model]

    reasoning_effort = body.get("reasoning_effort", None)
    effort = EFFORT_MAP.get(reasoning_effort, config["effort"])

    messages = body.get("messages", [])
    anthropic_messages = []
    system_content = None

    for msg in messages:
        role = msg.get("role", "user")
        content = msg.get("content", "")

        # contentê°€ listì¸ ê²½ìš° (multimodal) â†’ textë§Œ ì¶”ì¶œ
        if isinstance(content, list):
            text_parts = [p.get("text", "") for p in content if p.get("type") == "text"]
            content = "\n".join(text_parts) if text_parts else str(content)

        if role == "system":
            system_content = content if isinstance(content, str) else str(content)

        elif role == "assistant":
            # assistant + tool_calls â†’ Anthropic content blocks
            tool_calls_data = msg.get("tool_calls", [])
            if tool_calls_data:
                blocks = []
                if content:
                    blocks.append({"type": "text", "text": content})
                for tc in tool_calls_data:
                    func = tc.get("function", {})
                    try:
                        input_data = json.loads(func.get("arguments", "{}"))
                    except json.JSONDecodeError:
                        input_data = {"raw": func.get("arguments", "")}
                    blocks.append({
                        "type": "tool_use",
                        "id": tc.get("id", f"call_{int(time.time())}"),
                        "name": func.get("name", ""),
                        "input": input_data,
                    })
                anthropic_messages.append({"role": "assistant", "content": blocks})
            else:
                anthropic_messages.append({"role": "assistant", "content": content})

        elif role == "tool":
            # tool result â†’ Anthropic tool_result content block (user role)
            tool_call_id = msg.get("tool_call_id", "")
            result_content = content if isinstance(content, str) else json.dumps(content, ensure_ascii=False)
            tool_result_block = {
                "type": "tool_result",
                "tool_use_id": tool_call_id,
                "content": result_content,
            }
            # ì´ì „ ë©”ì‹œì§€ê°€ userì´ê³  contentê°€ listë©´ í•©ì¹¨
            if anthropic_messages and anthropic_messages[-1]["role"] == "user" and isinstance(anthropic_messages[-1]["content"], list):
                anthropic_messages[-1]["content"].append(tool_result_block)
            else:
                anthropic_messages.append({"role": "user", "content": [tool_result_block]})

        elif role == "user":
            anthropic_messages.append({"role": "user", "content": content})

    if not anthropic_messages:
        anthropic_messages = [{"role": "user", "content": "Hello"}]

    # Anthropic: user/assistant êµëŒ€ í•„ìˆ˜ â†’ ì—°ì† ê°™ì€ role í•©ì¹¨
    merged = []
    for msg in anthropic_messages:
        if merged and merged[-1]["role"] == msg["role"]:
            prev = merged[-1]["content"]
            curr = msg["content"]
            # ë‘˜ ë‹¤ stringì´ë©´ í•©ì¹¨
            if isinstance(prev, str) and isinstance(curr, str):
                merged[-1]["content"] = prev + "\n\n" + curr
            # ë‘˜ ë‹¤ listë©´ extend
            elif isinstance(prev, list) and isinstance(curr, list):
                prev.extend(curr)
            # í•˜ë‚˜ê°€ string, í•˜ë‚˜ê°€ list â†’ listë¡œ í†µí•©
            elif isinstance(prev, str) and isinstance(curr, list):
                merged[-1]["content"] = [{"type": "text", "text": prev}] + curr
            elif isinstance(prev, list) and isinstance(curr, str):
                prev.append({"type": "text", "text": curr})
        else:
            merged.append(dict(msg))
    anthropic_messages = merged

    max_tokens = (
        body.get("max_tokens")
        or body.get("max_completion_tokens")
        or config["max_tokens"]
    )
    # Antigravity 200K context cap â†’ max_tokens ì œí•œ
    max_tokens = min(max_tokens, 16000)

    anthropic_body = {
        "model": model,
        "max_tokens": max_tokens,
        "thinking": {"type": "adaptive", "effort": effort},
        "messages": anthropic_messages,
    }
    if system_content:
        anthropic_body["system"] = system_content

    # tools ë³€í™˜: OpenAI format â†’ Anthropic format (ìµœëŒ€ 64ê°œ, ì´ˆê³¼ ì‹œ ì˜ë¼ëƒ„)
    openai_tools = body.get("tools", [])
    if openai_tools:
        anthropic_tools = []
        for t in openai_tools:
            if t.get("type") == "function":
                func = t.get("function", {})
                anthropic_tools.append({
                    "name": func.get("name", ""),
                    "description": func.get("description", "")[:1024],
                    "input_schema": func.get("parameters", {"type": "object", "properties": {}}),
                })
        if anthropic_tools:
            anthropic_body["tools"] = anthropic_tools
            print(f"   ğŸ”§ Tools: {len(anthropic_tools)} forwarded")

    print(f"ğŸ” Thinking: model={model}, effort={effort}, msgs={len(anthropic_messages)}, stream={is_stream}")

    try:
        async with httpx.AsyncClient(timeout=300) as client:
            r = await client.post(
                f"{CCS_BASE}/v1/messages",
                json=anthropic_body,
                headers={
                    **CCS_HEADERS,
                    "anthropic-version": "2023-06-01",
                },
            )
    except Exception as e:
        print(f"âš ï¸ Thinking error: {e}")
        return JSONResponse(
            content={"error": {"message": str(e), "type": "proxy_error"}},
            status_code=502,
        )

    if r.status_code != 200:
        print(f"âš ï¸ Thinking upstream {r.status_code}: {r.text[:200]}")
        return JSONResponse(
            content={"error": {"message": f"Upstream: {r.text}", "type": "proxy_error"}},
            status_code=r.status_code,
        )

    result = r.json()

    # ğŸ’° Token usage logging
    usage = result.get("usage", {})
    input_tokens = usage.get("input_tokens", 0)
    output_tokens = usage.get("output_tokens", 0)
    cache_read = usage.get("cache_read_input_tokens", 0)
    cache_creation = usage.get("cache_creation_input_tokens", 0)
    print(f"ğŸ’° Usage: in={input_tokens} out={output_tokens} cache_read={cache_read} cache_create={cache_creation} total={input_tokens + output_tokens}")

    # Anthropic response â†’ thinking + text + tool_use ë¶„ë¦¬
    reasoning_content = None
    text_content = ""
    tool_calls = []

    for block in result.get("content", []):
        if block.get("type") == "thinking":
            reasoning_content = block.get("thinking", "")
        elif block.get("type") == "text":
            raw_text = block.get("text", "")
            thinking_match = re.search(r'<thinking>(.*?)</thinking>', raw_text, re.DOTALL)
            if thinking_match:
                reasoning_content = thinking_match.group(1).strip()
                text_content += re.sub(r'<thinking>.*?</thinking>\s*', '', raw_text, flags=re.DOTALL).strip()
            else:
                text_content += raw_text
        elif block.get("type") == "tool_use":
            tool_calls.append({
                "id": block.get("id", f"call_{int(time.time())}"),
                "type": "function",
                "function": {
                    "name": block.get("name", ""),
                    "arguments": json.dumps(block.get("input", {}), ensure_ascii=False),
                },
            })

    if not text_content and not reasoning_content and not tool_calls:
        text_content = "(No content returned)"

    # finish_reason: tool callì´ ìˆìœ¼ë©´ tool_calls, ì•„ë‹ˆë©´ stop
    finish_reason = "tool_calls" if tool_calls else "stop"

    completion_id = result.get("id", f"chatcmpl-{int(time.time())}")

    if is_stream:
        # SSE ìŠ¤íŠ¸ë¦¬ë° í¬ë§·ìœ¼ë¡œ ì‘ë‹µ
        return StreamingResponse(
            _thinking_to_sse(completion_id, model, text_content, reasoning_content, tool_calls, finish_reason),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no",
            },
        )
    else:
        # ì¼ë°˜ JSON ì‘ë‹µ
        msg = {"role": "assistant", "content": text_content}
        if reasoning_content:
            msg["reasoning_content"] = reasoning_content
        if tool_calls:
            msg["tool_calls"] = tool_calls

        return JSONResponse(content={
            "id": completion_id,
            "object": "chat.completion",
            "created": int(time.time()),
            "model": model,
            "choices": [{
                "index": 0,
                "message": msg,
                "finish_reason": finish_reason,
            }],
            "usage": {
                "prompt_tokens": result.get("usage", {}).get("input_tokens", 0),
                "completion_tokens": result.get("usage", {}).get("output_tokens", 0),
                "total_tokens": (
                    result.get("usage", {}).get("input_tokens", 0)
                    + result.get("usage", {}).get("output_tokens", 0)
                ),
            },
        })


async def _thinking_to_sse(completion_id: str, model: str, text: str, reasoning: str | None,
                           tool_calls: list | None = None, finish_reason: str = "stop"):
    """Anthropic ì‘ë‹µì„ OpenAI SSE chat.completion.chunk í¬ë§·ìœ¼ë¡œ ë³€í™˜"""
    created = int(time.time())

    def chunk(delta: dict, fr=None):
        return "data: " + json.dumps({
            "id": completion_id,
            "object": "chat.completion.chunk",
            "created": created,
            "model": model,
            "choices": [{
                "index": 0,
                "delta": delta,
                "finish_reason": fr,
            }],
        }, ensure_ascii=False) + "\n\n"

    # 1. role chunk
    yield chunk({"role": "assistant", "content": ""})

    # 2. contentë¥¼ ì‘ì€ ì¡°ê°ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë°
    if text:
        chunk_size = 20
        for i in range(0, len(text), chunk_size):
            yield chunk({"content": text[i:i + chunk_size]})

    # 3. tool_calls ìŠ¤íŠ¸ë¦¬ë° (ìˆì„ ê²½ìš°)
    if tool_calls:
        for idx, tc in enumerate(tool_calls):
            # ì²« chunk: tool call ì‹œì‘
            yield chunk({"tool_calls": [{
                "index": idx,
                "id": tc["id"],
                "type": "function",
                "function": {"name": tc["function"]["name"], "arguments": ""},
            }]})
            # argumentsë¥¼ ì¡°ê°ìœ¼ë¡œ ì „ì†¡
            args = tc["function"]["arguments"]
            args_chunk_size = 50
            for i in range(0, len(args), args_chunk_size):
                yield chunk({"tool_calls": [{
                    "index": idx,
                    "function": {"arguments": args[i:i + args_chunk_size]},
                }]})

    # 4. finish with usage
    yield chunk({}, fr=finish_reason)

    # 5. done
    yield "data: [DONE]\n\n"


# =====================================================================
# /v1/messages â€” Anthropic Messages API (Claude Code CLI ë“±)
# =====================================================================

@app.post("/v1/messages")
@app.post("/v1/messages/{path:path}")
async def messages(request: Request, path: str = ""):
    """Anthropic Messages API â€” Claude Code CLIì—ì„œ ì§ì ‘ ì‚¬ìš©"""
    # count_tokens ë“± ì„œë¸Œê²½ë¡œ â†’ CCSë¡œ ê·¸ëŒ€ë¡œ ì „ë‹¬
    if path:
        body = await request.json()
        url = f"{CCS_BASE}/v1/messages/{path}"
        qs = str(request.query_params)
        if qs:
            url += f"?{qs}"
        headers = {**CCS_HEADERS, "anthropic-version": "2023-06-01"}
        async with httpx.AsyncClient(timeout=30) as client:
            r = await client.post(url, json=body, headers=headers)
        return JSONResponse(content=r.json(), status_code=r.status_code)

    body = await request.json()
    model = body.get("model", "")
    is_stream = body.get("stream", False)

    # ëª¨ë¸ ë³„ì¹­ ì¹˜í™˜ (haiku â†’ sonnet 4.6 ë“±)
    if model in MODEL_ALIASES:
        original = model
        model = MODEL_ALIASES[model]
        body["model"] = model
        print(f"ğŸ“¨ [messages] {original} â†’ {model} stream={is_stream} msgs={len(body.get('messages',[]))}")
    else:
        print(f"ğŸ“¨ [messages] {model} stream={is_stream} msgs={len(body.get('messages',[]))}")

    # --- Route 1: Claude Thinking ëª¨ë¸ ---
    if model in THINKING_MODELS:
        return await _handle_thinking_messages(body, model, is_stream)

    # --- Route 2: Codex Effort ì ‘ë¯¸ì‚¬ ëª¨ë¸ ---
    match = EFFORT_SUFFIXES.match(model)
    if match:
        base_model = match.group(1)
        effort = match.group(2)
        return await _handle_codex_effort_messages(body, base_model, effort, is_stream)

    # --- Route 3: ì¼ë°˜ ëª¨ë¸ passthrough ---
    return await _messages_passthrough(f"{CCS_BASE}/v1/messages", body, is_stream)


async def _handle_thinking_messages(body: dict, model: str, is_stream: bool):
    """Claude thinking: thinking íŒŒë¼ë¯¸í„° ì‚½ì… â†’ CCS /v1/messages"""
    config = THINKING_MODELS[model]
    effort = body.get("thinking", {}).get("effort", config["effort"])

    # thinking íŒŒë¼ë¯¸í„° ì‚½ì… (ì—†ìœ¼ë©´ ì¶”ê°€, ìˆìœ¼ë©´ ìœ ì§€)
    if "thinking" not in body:
        body["thinking"] = {"type": "adaptive", "effort": effort}

    # max_tokens cap: Antigravity 200K context ì œí•œ
    if body.get("max_tokens", 0) > 16000:
        body["max_tokens"] = 16000

    print(f"ğŸ” [messages] Thinking: {model}, effort={effort}, stream={is_stream}")
    return await _messages_passthrough(f"{CCS_BASE}/v1/messages", body, is_stream)


async def _handle_codex_effort_messages(body: dict, base_model: str, effort: str, is_stream: bool):
    """Codex effort: ì ‘ë¯¸ì‚¬ íŒŒì‹± â†’ reasoning_effort ì‚½ì… â†’ Codex provider"""
    body["model"] = base_model
    body["reasoning_effort"] = effort

    print(f"ğŸ”§ [messages] Codex effort: {base_model} + {effort}, stream={is_stream}")
    return await _messages_passthrough(
        f"{CCS_BASE}/api/provider/codex/v1/messages", body, is_stream
    )


async def _messages_passthrough(url: str, body: dict, is_stream: bool):
    """Anthropic Messages í˜•ì‹ ìš”ì²­ì„ CCSë¡œ ì „ë‹¬ (ìŠ¤íŠ¸ë¦¬ë°/ë¹„ìŠ¤íŠ¸ë¦¬ë°)"""
    headers = {
        **CCS_HEADERS,
        "anthropic-version": "2023-06-01",
    }

    if is_stream:
        # SSE ìŠ¤íŠ¸ë¦¬ë°: CCSì˜ Anthropic SSEë¥¼ ê·¸ëŒ€ë¡œ ì „ë‹¬
        body["stream"] = True
        async def generate():
            async with httpx.AsyncClient(timeout=300) as client:
                async with client.stream(
                    "POST", url, json=body, headers=headers,
                ) as response:
                    async for line in response.aiter_lines():
                        if line:
                            yield line + "\n"
                        else:
                            yield "\n"
        return StreamingResponse(
            generate(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no",
            },
        )
    else:
        # ë¹„ìŠ¤íŠ¸ë¦¬ë°: JSON ì‘ë‹µ ê·¸ëŒ€ë¡œ ì „ë‹¬
        async with httpx.AsyncClient(timeout=300) as client:
            r = await client.post(url, json=body, headers=headers)

        if r.status_code != 200:
            print(f"âš ï¸ [messages] upstream {r.status_code}: {r.text[:200]}")
            return JSONResponse(content=r.json(), status_code=r.status_code)

        result = r.json()

        # ğŸ’° Token usage logging
        usage = result.get("usage", {})
        print(f"ğŸ’° Usage: in={usage.get('input_tokens',0)} out={usage.get('output_tokens',0)}")

        return JSONResponse(content=result, status_code=200)


@app.get("/health")
async def health():
    return {
        "status": "ok",
        "backend": CCS_BASE,
        "endpoints": {
            "/v1/chat/completions": ["thinking+sse", "codex-effort+sse", "passthrough+sse"],
            "/v1/messages": ["thinking", "codex-effort", "passthrough"],
        },
    }


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="CCS Wrapper (SSE + Messages)")
    parser.add_argument("--port", type=int, default=8318)
    parser.add_argument("--host", default="0.0.0.0")
    args = parser.parse_args()

    print(f"ğŸ§  CCS Wrapper Proxy starting on {args.host}:{args.port}")
    print(f"   Backend: {CCS_BASE}")
    print(f"   Endpoints: /v1/chat/completions, /v1/messages")
    print(f"   Claude thinking: {list(THINKING_MODELS.keys())}")
    print(f"   Codex effort: regex {EFFORT_SUFFIXES.pattern}")
    uvicorn.run(app, host=args.host, port=args.port, log_level="info")
